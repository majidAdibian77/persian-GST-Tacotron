
-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 16:14:59.530]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 16:14:59.530]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 16:14:59.530]  Using model: tacotron
[2022-01-04 16:14:59.530]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 10
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  use_cmudict: False
  use_gst: True

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 16:20:14.368]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 16:20:14.368]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 16:20:14.368]  Using model: tacotron
[2022-01-04 16:20:14.368]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 10
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  use_cmudict: False
  use_gst: True

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 21:19:42.019]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 21:19:42.019]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 21:19:42.019]  Using model: tacotron
[2022-01-04 21:19:42.019]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 16
  checkpoint_interval: 20
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 128
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  use_cmudict: False
  use_gst: True
[2022-01-04 21:19:42.022]  Loaded metadata for 90 examples (0.13 hours)
[2022-01-04 21:19:49.375]  Initialized Tacotron model. Dimensions: 
[2022-01-04 21:19:49.375]    text embedding:          256
[2022-01-04 21:19:49.375]    style embedding:         256
[2022-01-04 21:19:49.375]    prenet out:              128
[2022-01-04 21:19:49.375]    encoder out:             512
[2022-01-04 21:19:49.375]    attention out:           256
[2022-01-04 21:19:49.375]    concat attn & out:       768
[2022-01-04 21:19:49.375]    decoder cell out:        256
[2022-01-04 21:19:49.375]    decoder out (2 frames):  160
[2022-01-04 21:19:49.375]    decoder out (1 frame):   80
[2022-01-04 21:19:49.375]    postnet out:             256
[2022-01-04 21:19:49.375]    linear out:              1025
[2022-01-04 21:20:03.896]  Starting new training run at commit: None
[2022-01-04 21:20:07.892]  Generated 32 batches of size 16 in 3.996 sec
[2022-01-04 21:20:21.391]  Step 1       [17.494 sec/step, loss=0.96428, avg_loss=0.96428]
[2022-01-04 21:20:27.379]  Step 2       [11.741 sec/step, loss=0.95140, avg_loss=0.95784]
[2022-01-04 21:20:32.997]  Step 3       [9.700 sec/step, loss=0.97072, avg_loss=0.96213]
[2022-01-04 21:20:37.589]  Step 4       [8.423 sec/step, loss=0.94714, avg_loss=0.95838]

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 21:33:39.507]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 21:33:39.507]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 21:33:39.507]  Using model: tacotron
[2022-01-04 21:33:39.507]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 16
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 128
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  use_cmudict: False
  use_gst: True
[2022-01-04 21:33:39.509]  Loaded metadata for 90 examples (0.13 hours)

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 21:38:31.487]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 21:38:31.487]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 21:38:31.487]  Using model: tacotron
[2022-01-04 21:38:31.487]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 16
  checkpoint_interval: 20
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 128
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  summary_interval: 10
  use_cmudict: False
  use_gst: True
[2022-01-04 21:38:31.489]  Loaded metadata for 90 examples (0.13 hours)
[2022-01-04 21:38:38.682]  Initialized Tacotron model. Dimensions: 
[2022-01-04 21:38:38.682]    text embedding:          256
[2022-01-04 21:38:38.682]    style embedding:         256
[2022-01-04 21:38:38.682]    prenet out:              128
[2022-01-04 21:38:38.682]    encoder out:             512
[2022-01-04 21:38:38.682]    attention out:           256
[2022-01-04 21:38:38.682]    concat attn & out:       768
[2022-01-04 21:38:38.682]    decoder cell out:        256
[2022-01-04 21:38:38.682]    decoder out (2 frames):  160
[2022-01-04 21:38:38.682]    decoder out (1 frame):   80
[2022-01-04 21:38:38.682]    postnet out:             256
[2022-01-04 21:38:38.682]    linear out:              1025
[2022-01-04 21:38:53.790]  Starting new training run at commit: None
[2022-01-04 21:38:57.738]  Generated 32 batches of size 16 in 3.948 sec
[2022-01-04 21:39:05.743]  Exiting due to exception: 'Namespace' object has no attribute 'checkpoint_interval'

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2022-01-04 21:41:17.868]  Checkpoint path: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt
[2022-01-04 21:41:17.868]  Loading training data from: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/training/train.txt
[2022-01-04 21:41:17.868]  Using model: tacotron
[2022-01-04 21:41:17.868]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 16
  checkpoint_interval: 20
  cleaners: english_cleaners
  decay_learning_rate: True
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 1000
  min_level_db: -100
  num_freq: 1025
  num_gst: 128
  num_heads: 4
  num_mels: 80
  outputs_per_step: 2
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  reference_depth: 128
  reference_filters: [32, 32, 64, 64, 128, 128]
  rnn_depth: 256
  sample_rate: 16000
  style_att_dim: 128
  style_att_type: mlp_attention
  style_embed_depth: 256
  summary_interval: 10
  use_cmudict: False
  use_gst: True
[2022-01-04 21:41:17.871]  Loaded metadata for 90 examples (0.13 hours)
[2022-01-04 21:41:24.821]  Initialized Tacotron model. Dimensions: 
[2022-01-04 21:41:24.821]    text embedding:          256
[2022-01-04 21:41:24.821]    style embedding:         256
[2022-01-04 21:41:24.821]    prenet out:              128
[2022-01-04 21:41:24.821]    encoder out:             512
[2022-01-04 21:41:24.821]    attention out:           256
[2022-01-04 21:41:24.822]    concat attn & out:       768
[2022-01-04 21:41:24.822]    decoder cell out:        256
[2022-01-04 21:41:24.822]    decoder out (2 frames):  160
[2022-01-04 21:41:24.822]    decoder out (1 frame):   80
[2022-01-04 21:41:24.822]    postnet out:             256
[2022-01-04 21:41:24.822]    linear out:              1025
[2022-01-04 21:41:38.884]  Starting new training run at commit: None
[2022-01-04 21:41:42.729]  Generated 32 batches of size 16 in 3.845 sec
[2022-01-04 21:41:51.430]  Step 1       [12.545 sec/step, loss=0.97189, avg_loss=0.97189]
[2022-01-04 21:42:02.030]  Step 2       [11.572 sec/step, loss=0.93305, avg_loss=0.95247]
[2022-01-04 21:42:05.597]  Step 3       [8.904 sec/step, loss=0.97767, avg_loss=0.96087]
[2022-01-04 21:42:10.370]  Step 4       [7.871 sec/step, loss=0.96191, avg_loss=0.96113]
[2022-01-04 21:42:27.115]  Step 5       [9.646 sec/step, loss=0.95460, avg_loss=0.95982]
[2022-01-04 21:42:31.546]  Step 6       [8.777 sec/step, loss=0.98668, avg_loss=0.96430]
[2022-01-04 21:42:43.142]  Step 7       [9.179 sec/step, loss=0.97070, avg_loss=0.96522]
[2022-01-04 21:42:47.743]  Step 8       [8.607 sec/step, loss=0.99173, avg_loss=0.96853]
[2022-01-04 21:42:50.875]  Step 9       [7.999 sec/step, loss=0.94708, avg_loss=0.96615]
[2022-01-04 21:42:58.004]  Step 10      [7.912 sec/step, loss=0.96563, avg_loss=0.96609]
[2022-01-04 21:42:58.004]  Writing summary at step: 10
[2022-01-04 21:43:22.804]  Step 11      [7.987 sec/step, loss=0.93283, avg_loss=0.96307]
[2022-01-04 21:43:29.657]  Step 12      [7.893 sec/step, loss=0.94256, avg_loss=0.96136]
[2022-01-04 21:43:32.262]  Step 13      [7.486 sec/step, loss=0.94356, avg_loss=0.95999]
[2022-01-04 21:43:43.130]  Step 14      [7.727 sec/step, loss=0.94291, avg_loss=0.95877]
[2022-01-04 21:43:48.370]  Step 15      [7.562 sec/step, loss=0.96099, avg_loss=0.95892]
[2022-01-04 21:43:49.900]  Step 16      [7.185 sec/step, loss=0.84939, avg_loss=0.95207]
[2022-01-04 21:43:53.201]  Step 17      [6.956 sec/step, loss=0.96006, avg_loss=0.95254]
[2022-01-04 21:44:02.523]  Step 18      [7.088 sec/step, loss=0.96996, avg_loss=0.95351]
[2022-01-04 21:44:04.286]  Step 19      [6.807 sec/step, loss=0.86210, avg_loss=0.94870]
[2022-01-04 21:44:07.720]  Step 20      [6.639 sec/step, loss=0.95257, avg_loss=0.94889]
[2022-01-04 21:44:07.720]  Writing summary at step: 20
[2022-01-04 21:44:13.776]  Saving checkpoint to: /content/drive/My Drive/Projects/TTS/multi speaker/GST_Tacotron/gst-tacotron/logs-tacotron/model.ckpt-20
[2022-01-04 21:44:15.663]  Saving audio and alignment...
[2022-01-04 21:44:23.414]  Exiting due to exception: module 'librosa' has no attribute 'output'
